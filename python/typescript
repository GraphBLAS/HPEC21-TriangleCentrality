Script started on Tue 10 Aug 2021 12:29:04 PM CDT
groups: cannot find name for group ID 8047
Intel Suite:
Copyright (C) 2009-2019 Intel Corporation. All rights reserved.
Intel(R) VTune(TM) Amplifier 2019 (build 591499)
[1;34m==>[0m 75 installed packages
-- [0;35mlinux-ubuntu16.04-broadwell[0m / [0;32mgcc@5.4.0[0m ----------------------
autoconf[0;36m@2.69[0m                gcc[0;36m@9.3.0[0m             libevent[0;36m@2.1.12[0m      mpfr[0;36m@3.1.6[0m      py-setuptools[0;36m@46.1.3[0m
autoconf[0;36m@2.69[0m                gcc[0;36m@10.1.0[0m            libffi[0;36m@3.3[0m           mpfr[0;36m@4.0.2[0m      python[0;36m@3.7.7[0m
autoconf-archive[0;36m@2019.01.06[0m  gdbm[0;36m@1.18.1[0m           libffi[0;36m@3.3[0m           ncurses[0;36m@6.2[0m     python[0;36m@3.8.0[0m
automake[0;36m@1.16.2[0m              gettext[0;36m@0.20.2[0m        libiconv[0;36m@1.16[0m        numactl[0;36m@2.0.14[0m  python[0;36m@3.8.7[0m
automake[0;36m@1.16.3[0m              gettext[0;36m@0.21[0m          libidn2[0;36m@2.1.1a[0m       openmpi[0;36m@4.0.5[0m   readline[0;36m@8.0[0m
berkeley-db[0;36m@18.1.40[0m          git[0;36m@2.26.0[0m            libidn2[0;36m@2.3.0[0m        openssh[0;36m@8.4p1[0m   sqlite[0;36m@3.31.1[0m
boost[0;36m@1.75.0[0m                 git[0;36m@2.29.0[0m            libpciaccess[0;36m@0.16[0m    openssl[0;36m@1.1.1g[0m  sqlite[0;36m@3.34.0[0m
bzip2[0;36m@1.0.8[0m                  gmp[0;36m@6.1.2[0m             libsigsegv[0;36m@2.12[0m      openssl[0;36m@1.1.1j[0m  tar[0;36m@1.32[0m
cmake[0;36m@3.19.5[0m                 gmp[0;36m@6.2.1[0m             libtool[0;36m@2.4.6[0m        pcre2[0;36m@10.31[0m     util-linux-uuid[0;36m@2.36[0m
curl[0;36m@7.68.0[0m                  hwloc[0;36m@2.4.0[0m           libunistring[0;36m@0.9.10[0m  pcre2[0;36m@10.35[0m     util-macros[0;36m@1.19.1[0m
curl[0;36m@7.74.0[0m                  isl[0;36m@0.18[0m              libxml2[0;36m@2.9.10[0m       perl[0;36m@5.30.2[0m     valgrind[0;36m@3.16.1[0m
diffutils[0;36m@3.7[0m                isl[0;36m@0.20[0m              libxml2[0;36m@2.9.10[0m       perl[0;36m@5.32.1[0m     xz[0;36m@5.2.5[0m
expat[0;36m@2.2.9[0m                  isl[0;36m@0.21[0m              m4[0;36m@1.4.18[0m            pkgconf[0;36m@1.6.3[0m   xz[0;36m@5.2.5[0m
expat[0;36m@2.2.10[0m                 libbsd[0;36m@0.10.0[0m         mpc[0;36m@1.1.0[0m            pkgconf[0;36m@1.7.3[0m   zlib[0;36m@1.2.11[0m
gcc[0;36m@6.3.0[0m                    libedit[0;36m@3.1-20191231[0m  mpc[0;36m@1.1.0[0m            py-pip[0;36m@19.3[0m     zstd[0;36m@1.4.4[0m
[0;36mhypersparse $[0m 
[0;36mhypersparse $[0m 
[0;36mhypersparse $[0m python --version
Python 3.8.0
[0;36mhypersparse $[0m which ypth[K[K[K[Kpython
/home/faculty/davis/GrB/pygraphblas_test/bin/python
[0;36mhypersparse $[0m 
[0;36mhypersparse $[0m 
[0;36mhypersparse $[0m pwd
/home/faculty/davis/GrB/HPEC21-TriangleCentrality/python
[0;36mhypersparse $[0m dir
total 4
4 tc.py  0 typescript
[0;36mhypersparse $[0m python tc.py
Loading Newman/karate
Newman/karate | (34, 34) | 156 edges | 45 triangles
Running TC1 on Newman/karate 3 times
TC1 on Newman/karate took  0.0003325830524166425 average for 3 runs
Running TC3 on Newman/karate 3 times
TC3 on Newman/karate took  0.00043916764358679455 average for 3 runs
TC1 equal to TC3? True
Loading SNAP/com-Youtube
Traceback (most recent call last):
  File "tc.py", line 77, in <module>
    G = dict(Matrix.ssget(name, binary_cache_dir="~/.ssgetpy"))[
  File "/home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages/pygraphblas/matrix.py", line 560, in ssget
    M.to_binfile(Mbin)
  File "/home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages/pygraphblas/matrix.py", line 871, in binwrite
    binary.binwrite(self._matrix, filename, comments, opener)
  File "/home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages/suitesparse_graphblas/io/binary.py", line 300, in binwrite
    fwrite(buff(Ax[0], Ax_size[0]))
OSError: [Errno 14] Bad address
[0;36mhypersparse $[0m pwd
/home/faculty/davis/GrB/HPEC21-TriangleCentrality/python
[0;36mhypersparse $[0m vi /home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages/suitesparse_graphblas/io/binar y.py
[?1000h[?1049h[?1h=[2;1Hâ–½[6n[2;1H  [1;1H[1;50r[?12;25h[?12l[?25h[27m[23m[m[97m[40m[H[2J[?25l[50;1H"~/GrB/pygraphblas_test/lib/python3.8/site-packages/suitesparse_graphblas/io/binary.py"<rB/pygraphblas_test/lib/python3.8/site-packages/suitesparse_graphblas/io/binary.py" 558L, 16589C[>c[1;1H[95mfrom[m[97m[40m pathlib [95mimport[m[97m[40m Path
[95mfrom[m[97m[40m ctypes.util [95mimport[m[97m[40m find_library
[95mfrom[m[97m[40m suitesparse_graphblas [95mimport[m[97m[40m ffi, lib, check_status, matrix, __version__
[95mfrom[m[97m[40m cffi [95mimport[m[97m[40m FFI


stdffi = FFI()
stdffi.cdef(
    [91m"""
void *malloc(size_t size);
"""[m[97m[40m
)
stdlib = stdffi.dlopen(find_library([91m"c"[m[97m[40m))

[96m# When "packing" a matrix the owner of the memory buffer is transfered
# to SuiteSparse, which then becomes responsible for freeing it.  cffi
# wisely does not allow you to do this without declaring and calling
# malloc directly.  When SuiteSparse moves over to a more formal
# memory manager with the cuda work, this will likely change and have
# to be replaceable with a allocator common to numpy, cuda, and here.
# Maybe PyDataMem_NEW?[m[97m[40m


[92mdef[m[97m[40m [1m[95mreadinto_new_buffer[m[97m[40m(f, typ, size, allocator=stdlib.malloc):
    buff = ffi.cast(typ, allocator(size))
    f.readinto(ffi.buffer(buff, size))
    [92mreturn[m[97m[40m buff


GRB_HEADER_LEN = [92m512[m[97m[40m
NULL = ffi.NULL

header_template = [91m"""[m[97m[40m[91m[40m\[m[97m[40m
[91mSuiteSparse:GraphBLAS matrix
{suitesparse_version} ({user_agent})
nrows:   {nrows}
ncols:   {ncols}
nvec:    {nvec}
nvals:   {nvals}
format:  {format}
size:    {size}
type:    {type}
iso:     {iso}
jumbled: {jumbled}
{comments}
"""[m[97m[40m

sizeof = ffi.sizeof
ffinew = ffi.new[50;99H1,1[11CTop[1;1H[?12l[?25h[?1000l[?1006h[?1002h[?1006l[?1002l[?1006h[?1002hP+q436f\P+q6b75\P+q6b64\P+q6b72\P+q6b6c\P+q2332\P+q2334\P+q2569\P+q2a37\P+q6b31\[50;1H[?1006l[?1002l[50;1H[K[50;1H[?1l>[?1049l[0;36mhypersparse $[0m 
[0;36mhypersparse $[0m 
[0;36mhypersparse $[0m vi virtualenv --python=python3.8 pygraphblas_test
VIM - Vi IMproved 7.4 (2013 Aug 10, compiled Oct 13 2020 16:04:38)
Unknown option argument: "--python=python3.8"
More info with: "vim -h"
[0;36mhypersparse $[0m     . pygraphblas_test/bin/activate
bash: pygraphblas_test/bin/activate: No such file or directory
[0;36mhypersparse $[0m     pip3 install pygraphblas
Requirement already satisfied: pygraphblas in /home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages (5.1.5.1)
Requirement already satisfied: scipy in /home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages (from pygraphblas) (1.7.1)
Requirement already satisfied: suitesparse-graphblas in /home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages (from pygraphblas) (5.1.5.0)
Requirement already satisfied: numba in /home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages (from pygraphblas) (0.53.1)
Requirement already satisfied: contextvars in /home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages (from pygraphblas) (2.4)
Requirement already satisfied: immutables>=0.9 in /home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages (from contextvars->pygraphblas) (0.16)
Requirement already satisfied: setuptools in /home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages (from numba->pygraphblas) (57.4.0)
Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages (from numba->pygraphblas) (0.36.0)
Requirement already satisfied: numpy>=1.15 in /home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages (from numba->pygraphblas) (1.21.1)
Requirement already satisfied: cffi>=1.0.0 in /home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages (from suitesparse-graphblas->pygraphblas) (1.14.6)
Requirement already satisfied: pycparser in /home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages (from cffi>=1.0.0->suitesparse-graphblas->pygraphblas) (2.20)
[0;36mhypersparse $[0m     python3 -c 'import pygraphblas; print(pygraphblas.GxB_IMPLEMENTATION)'
(5, 1, 5)
[0;36mhypersparse $[0m 
[0;36mhypersparse $[0m 
[0;36mhypersparse $[0m pwd
/home/faculty/davis/GrB/HPEC21-TriangleCentrality/python
[0;36mhypersparse $[0m vi /home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages/suitesparse_graphblas/io/binar y.py
[?1000h[?1049h[?1h=[2;1Hâ–½[6n[2;1H  [1;1H[1;50r[?12;25h[?12l[?25h[27m[23m[m[97m[40m[H[2J[?25l[50;1H"~/GrB/pygraphblas_test/lib/python3.8/site-packages/suitesparse_graphblas/io/binary.py"<rB/pygraphblas_test/lib/python3.8/site-packages/suitesparse_graphblas/io/binary.py" 558L, 16589C[>c[1;1H[95mfrom[m[97m[40m pathlib [95mimport[m[97m[40m Path
[95mfrom[m[97m[40m ctypes.util [95mimport[m[97m[40m find_library
[95mfrom[m[97m[40m suitesparse_graphblas [95mimport[m[97m[40m ffi, lib, check_status, matrix, __version__
[95mfrom[m[97m[40m cffi [95mimport[m[97m[40m FFI


stdffi = FFI()
stdffi.cdef(
    [91m"""
void *malloc(size_t size);
"""[m[97m[40m
)
stdlib = stdffi.dlopen(find_library([91m"c"[m[97m[40m))

[96m# When "packing" a matrix the owner of the memory buffer is transfered
# to SuiteSparse, which then becomes responsible for freeing it.  cffi
# wisely does not allow you to do this without declaring and calling
# malloc directly.  When SuiteSparse moves over to a more formal
# memory manager with the cuda work, this will likely change and have
# to be replaceable with a allocator common to numpy, cuda, and here.
# Maybe PyDataMem_NEW?[m[97m[40m


[92mdef[m[97m[40m [1m[95mreadinto_new_buffer[m[97m[40m(f, typ, size, allocator=stdlib.malloc):
    buff = ffi.cast(typ, allocator(size))
    f.readinto(ffi.buffer(buff, size))
    [92mreturn[m[97m[40m buff


GRB_HEADER_LEN = [92m512[m[97m[40m
NULL = ffi.NULL

header_template = [91m"""[m[97m[40m[91m[40m\[m[97m[40m
[91mSuiteSparse:GraphBLAS matrix
{suitesparse_version} ({user_agent})
nrows:   {nrows}
ncols:   {ncols}
nvec:    {nvec}
nvals:   {nvals}
format:  {format}
size:    {size}
type:    {type}
iso:     {iso}
jumbled: {jumbled}
{comments}
"""[m[97m[40m

sizeof = ffi.sizeof
ffinew = ffi.new[50;99H1,1[11CTop[1;1H[?12l[?25h[?1000l[?1006h[?1002h[?1006l[?1002l[?1006h[?1002hP+q436f\P+q6b75\P+q6b64\P+q6b72\P+q6b6c\P+q2332\P+q2334\P+q2569\P+q2a37\P+q6b31\[?25l[27m[23m[m[97m[40m[H[2J[1;9Hfwrite(buff(typecode, sizeof([91m"int32_t"[m[97m[40m)))[2;9Hfwrite(buff(typesize, sizeof([91m"size_t"[m[97m[40m)))[3;9Hfwrite(buff(is_iso, sizeof([91m"bool"[m[97m[40m)))[4;9Hfwrite(buff(is_jumbled, sizeof([91m"bool"[m[97m[40m)))[6;9H[92mif[m[97m[40m is_hyper:[7;13HAp_size[[92m0[m[97m[40m] = (nvec[[92m0[m[97m[40m] + [92m1[m[97m[40m) * Isize[8;13HAh_size[[92m0[m[97m[40m] = nvec[[92m0[m[97m[40m] * Isize[9;13HAi_size[[92m0[m[97m[40m] = nvals[[92m0[m[97m[40m] * Isize[10;13HAx_size[[92m0[m[97m[40m] = nvals[[92m0[m[97m[40m] * typesize[[92m0[m[97m[40m][11;13Hfwrite(buff(Ap[[92m0[m[97m[40m], Ap_size[[92m0[m[97m[40m]))[12;13Hfwrite(buff(Ah[[92m0[m[97m[40m], Ah_size[[92m0[m[97m[40m]))[13;13Hfwrite(buff(Ai[[92m0[m[97m[40m], Ai_size[[92m0[m[97m[40m]))[14;9H[92melif[m[97m[40m is_sparse:[15;13HAp_size[[92m0[m[97m[40m] = (nvec[[92m0[m[97m[40m] + [92m1[m[97m[40m) * Isize[16;13HAi_size[[92m0[m[97m[40m] = nvals[[92m0[m[97m[40m] * Isize[17;13HAx_size[[92m0[m[97m[40m] = nvals[[92m0[m[97m[40m] * typesize[[92m0[m[97m[40m][18;13Hfwrite(buff(Ap[[92m0[m[97m[40m], Ap_size[[92m0[m[97m[40m]))[19;13Hfwrite(buff(Ai[[92m0[m[97m[40m], Ai_size[[92m0[m[97m[40m]))[20;9H[92melif[m[97m[40m is_bitmap:[21;13HAb_size[[92m0[m[97m[40m] = nrows[[92m0[m[97m[40m] * ncols[[92m0[m[97m[40m] * ffi.sizeof([91m"int8_t"[m[97m[40m)[22;13HAx_size[[92m0[m[97m[40m] = nrows[[92m0[m[97m[40m] * ncols[[92m0[m[97m[40m] * typesize[[92m0[m[97m[40m][23;13Hfwrite(buff(Ab[[92m0[m[97m[40m], Ab_size[[92m0[m[97m[40m]))[25;9Hfwrite(buff(Ax[[92m0[m[97m[40m], Ax_size[[92m0[m[97m[40m]))[27;5H[92mif[m[97m[40m by_col [92mand[m[97m[40m is_hyper:[28;9Hcheck_status([29;13HA,[30;13Hlib.GxB_Matrix_pack_HyperCSC([31;17HA[[92m0[m[97m[40m],[32;17HAp,[33;17HAh,[34;17HAi,[35;17HAx,[36;17HAp_size[[92m0[m[97m[40m],[37;17HAh_size[[92m0[m[97m[40m],[38;17HAi_size[[92m0[m[97m[40m],[39;17HAx_size[[92m0[m[97m[40m],[40;17His_iso[[92m0[m[97m[40m],[41;17Hnvec[[92m0[m[97m[40m],[42;17His_jumbled[[92m0[m[97m[40m],[43;17HNULL,[44;13H),[45;9H)[47;5H[92melif[m[97m[40m by_row [92mand[m[97m[40m is_hyper:[48;9Hcheck_status([49;13HA,[50;99H300,9[9C54%[25;9H[?12l[?25h[50;1H[?1006l[?1002l[50;99H[K[50;1H[?1l>[?1049l[0;36mhypersparse $[0m 
[0;36mhypersparse $[0m 
[0;36mhypersparse $[0m 
[0;36mhypersparse $[0m 
[0;36mhypersparse $[0m 
[0;36mhypersparse $[0m vi /home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages/suitesparse_graphblas/io/binaryy.py[A[0;36mhypersparse $[0m pwd[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvi virtualenv --python=python3.8 pygraphblas_test[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C/home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages/suitesparse_graphblas/io/binaryy.py[A[0;36mhypersparse $[0m pwd[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cython tc.pydir[Kpython tc.py
Loading Newman/karate
Newman/karate | (34, 34) | 156 edges | 45 triangles
Running TC1 on Newman/karate 3 times
TC1 on Newman/karate took  0.0003313074509302775 average for 3 runs
Running TC3 on Newman/karate 3 times
TC3 on Newman/karate took  0.0004579400022824605 average for 3 runs
TC1 equal to TC3? True
Loading SNAP/com-Youtube
SNAP/com-Youtube | (1134890, 1134890) | 5975248 edges | 3056386 triangles
Running TC1 on SNAP/com-Youtube 3 times
TC1 on SNAP/com-Youtube took  0.1590719468270739 average for 3 runs
Running TC3 on SNAP/com-Youtube 3 times
TC3 on SNAP/com-Youtube took  0.11261013305435578 average for 3 runs
TC1 equal to TC3? True
Loading SNAP/as-Skitter
SNAP/as-Skitter | (1696415, 1696415) | 22190596 edges | 28769868 triangles
Running TC1 on SNAP/as-Skitter 3 times
TC1 on SNAP/as-Skitter took  0.5855515686174234 average for 3 runs
Running TC3 on SNAP/as-Skitter 3 times
TC3 on SNAP/as-Skitter took  0.363020408898592 average for 3 runs
TC1 equal to TC3? True
Loading SNAP/com-LiveJournal
SNAP/com-LiveJournal | (3997962, 3997962) | 69362378 edges | 177820130 triangles
Running TC1 on SNAP/com-LiveJournal 3 times
TC1 on SNAP/com-LiveJournal took  2.251849979472657 average for 3 runs
Running TC3 on SNAP/com-LiveJournal 3 times
TC3 on SNAP/com-LiveJournal took  1.1641074701522787 average for 3 runs
TC1 equal to TC3? True
Loading SNAP/com-Orkut
SNAP/com-Orkut | (3072441, 3072441) | 234370166 edges | 627584181 triangles
Running TC1 on SNAP/com-Orkut 3 times
TC1 on SNAP/com-Orkut took  19.57805084840705 average for 3 runs
Running TC3 on SNAP/com-Orkut 3 times
TC3 on SNAP/com-Orkut took  9.397451189657053 average for 3 runs
TC1 equal to TC3? True
Loading SNAP/com-Friendster
Traceback (most recent call last):
  File "tc.py", line 77, in <module>
    G = dict(Matrix.ssget(name, binary_cache_dir="~/.ssgetpy"))[
  File "/home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages/pygraphblas/matrix.py", line 560, in ssget
    M.to_binfile(Mbin)
  File "/home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages/pygraphblas/matrix.py", line 871, in binwrite
    binary.binwrite(self._matrix, filename, comments, opener)
  File "/home/faculty/davis/GrB/pygraphblas_test/lib/python3.8/site-packages/suitesparse_graphblas/io/binary.py", line 300, in binwrite
    fwrite(buff(Ax[0], Ax_size[0]))
OSError: [Errno 14] Bad address
[0;36mhypersparse $[0m [K[0;36mhypersparse $[0m e[Kexit
exit

Script done on Tue 10 Aug 2021 04:28:16 PM CDT
